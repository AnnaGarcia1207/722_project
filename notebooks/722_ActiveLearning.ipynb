{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from gensim import corpora, similarities\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.similarities import Similarity\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import preprocessor as tp\n",
    "\n",
    "#from imblearn.ensemble import BalancedBaggingClassifier\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from random import randint\n",
    "import queue\n",
    "import pickle\n",
    "import os\n",
    "from multiprocessing import Pool as ProcessPool\n",
    "import itertools\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianllinas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#tp.set_options(tp.OPT.URL, tp.OPT.MENTION)\n",
    "\n",
    "rng = np.random.seed(5)\n",
    "random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def active_learning_multi_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(p, q):\n",
    "    if p == 0 or q == 0:\n",
    "        return 0\n",
    "    return -1*(p*math.log(p, 2) + q*math.log(q, 2))\n",
    "\n",
    "# actual active learning for TREC is happening here for a particular topicID\n",
    "# here we run for either all documents in the collection\n",
    "# or all documents in the official qrels\n",
    "\n",
    "def active_learning_multi_processing(topicID, df, al_protocol, al_classifier, document_collection, topic_seed_info, \n",
    "                                     topic_complete_qrels_address, train_per_centage, use_pooled_budget, \n",
    "                                     per_topic_budget_from_trec_qrels, feature_type):\n",
    "    train_index_list = topic_seed_info[topicID]\n",
    "    \n",
    "    original_labels = {}\n",
    "    for row_index, row in df.iterrows():\n",
    "        original_labels[row_index] = row['majority_label']\n",
    "\n",
    "    original_predicted_merged_dict = {}\n",
    "    original_label_list = []\n",
    "    number_of_1 = 0\n",
    "    for k, v in original_labels.items():\n",
    "        original_predicted_merged_dict[k] = v\n",
    "        if v == 1.0:\n",
    "            number_of_1 += 1\n",
    "        original_label_list.append(v)\n",
    "\n",
    "    original_predicted_merged_list = []\n",
    "    for k in sorted(original_predicted_merged_dict.keys()):\n",
    "        original_predicted_merged_list.append(original_predicted_merged_dict[k])\n",
    "\n",
    "    # need to convert y to np.array the Y otherwise Y[train_index_list] does not work directly on a list\n",
    "    y = np.array(original_predicted_merged_list)\n",
    "    # type needed beacause y is an object need and thorws error Unknown label type: 'unknown'\n",
    "    y = y.astype('int')\n",
    "\n",
    "    total_documents = len(y)\n",
    "    total_document_set = set(np.arrange(0, total_documents, 1))\n",
    "\n",
    "    initial_X_test = []\n",
    "    test_index_dictionary = {}\n",
    "    test_index_counter = 0\n",
    "\n",
    "    for train_index in range(0, total_documents):\n",
    "        if train_index not in train_index_list:\n",
    "            initial_X_test.append(document_collection[train_index])\n",
    "            test_index_dictionary[test_index_counter] = train_index\n",
    "            test_index_counter += 1\n",
    "\n",
    "    predicatbleSize = len(initial_X_test)\n",
    "    isPredictable = [1] * predicatbleSize # initially we will predict all\n",
    "\n",
    "    # initializing the train_size controller\n",
    "    train_size_controller = len(train_index_list)\n",
    "    loopCounter = 1 # loop starts from 1 because 0 is for seed_set\n",
    "    topic_all_info = {} # key is the loopCounter\n",
    "\n",
    "    while True:\n",
    "        #print \"iteration:\", loopCounter\n",
    "        # here modeling is utilizing the document selected in previous\n",
    "        # iteration for training\n",
    "        # when loopCounter == 0\n",
    "        # model is utilizing all the seed document collected at the begining\n",
    "        if al_classifier == 'LR':\n",
    "            #model = LogisticRegression(solver=large_data_solver, C=large_data_C_parameter, max_iter=200)\n",
    "            model = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "        elif al_classifier == 'SVM':\n",
    "            model = SVC(C=1.0, kernel='linear', degree=3, gamma='auto', probability = True)\n",
    "        elif al_classifier == 'RF':\n",
    "            model =  RandomForestClassifier(n_estimators=10, max_depth=10, random_state=0)\n",
    "        elif al_classifier == 'RFN':\n",
    "            model = RandomForestClassifier(n_estimators=10, max_depth=None, random_state=0)\n",
    "        elif al_classifier == 'RFN100':\n",
    "            model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=0)\n",
    "        elif al_classifier == 'NB':\n",
    "            model = MultinomialNB()\n",
    "        elif al_classifier == 'Ada':\n",
    "            # base model is decision tree\n",
    "            # logistic regression will not help\n",
    "            model = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "        elif al_classifier == 'Xgb':\n",
    "            model = XGBClassifier(random_state=1, learning_rate=0.01)\n",
    "        elif al_classifier == 'BagLR':\n",
    "            LRmodel = LogisticRegression(solver=large_data_solver, C=large_data_C_parameter, max_iter=200)\n",
    "            model = BaggingClassifier(LRmodel, n_estimators = 5, max_samples = 1) # If float, then draw max_samples * X.shape[0] samples. 1 means use all samples\n",
    "        elif al_classifier == 'BagNB':\n",
    "            model = BaggingClassifier(MultinomialNB(), n_estimators = 5, max_samples = 0.5) # If float, then draw max_samples * X.shape[0] samples. 1 means use all samples\n",
    "        elif al_classifier == 'Vot':\n",
    "            LRmodel = LogisticRegression(solver=large_data_solver, C=large_data_C_parameter, max_iter=200)\n",
    "            NBmodel = MultinomialNB()\n",
    "            model = VotingClassifier(estimators=[('lr', LRmodel), ('nb', NBmodel)], voting = 'soft')\n",
    "\n",
    "        model.fit(document_collection[train_index_list], y[train_index_list])\n",
    "\n",
    "        test_index_list = list(total_document_set - set(train_index_list))\n",
    "        pooled_document_count = len(set(train_index_list).intersection(set(original_label_list)))\n",
    "\n",
    "        y_actual = None\n",
    "        y_pred = None\n",
    "        y_pred_all = []\n",
    "\n",
    "        if isPredictable.count(1) != 0:\n",
    "            y_pred = []\n",
    "            for test_index_elem in test_index_list:\n",
    "                if feature_type == 'tfidf':\n",
    "                    y_pred.append(model.predict(document_collection[test_index_elem]))\n",
    "                elif feature_type == 'bert' or feature_type == 'robert':\n",
    "                    y_pred.append(model.predict(document_collection[test_index_elem]))\n",
    "\n",
    "            start = time.time()\n",
    "            y_actual = np.concatenate((y[train_index_list], y[test_index_list]), axis=None)\n",
    "            y_pred_all = np.concatenate((y[train_index_list], y_pred), axis=None)\n",
    "            '''\n",
    "            for doc_index in range(0,total_documents):\n",
    "                if doc_index in train_index_list:\n",
    "                    y_pred_all.append(y[doc_index])\n",
    "                else:\n",
    "                    # result_index in test_set\n",
    "                    # test_index_list is a list of doc_index\n",
    "                    # test_Index_list [25, 9, 12]\n",
    "                    # test_index_list[0] = 25 and its prediction in y_pred[0] --one to one mapping\n",
    "                    # so find the index of doc_index in test_index_list using\n",
    "                    pred_index = test_index_list.index(doc_index)\n",
    "                    y_pred_all.append(y_pred[pred_index])\n",
    "            '''\n",
    "\n",
    "        else: # everything in trainset\n",
    "            y_pred = y\n",
    "            y_actual = y\n",
    "            y_pred_all = y\n",
    "            test_index_list = train_index_list\n",
    "\n",
    "        f1score = f1_score(y_actual, y_pred_all, average='binary')\n",
    "        precision = precision_score(y_actual, y_pred_all, average='binary')\n",
    "        recall = recall_score(y_actual, y_pred_all, average='binary')\n",
    "\n",
    "        number_of_1_found_so_far = list(y[train_index_list]).count(1)\n",
    "        prevalence = (number_of_1_found_so_far*1.0)/number_of_1\n",
    "\n",
    "        # save all info using (loopCounter - 1) \n",
    "        # list should be deep_copy otherwise all will point to final reference at final iteration\n",
    "        topic_all_info[loopCounter - 1] = (topicID, f1score, precision, recall, copy.deepcopy(train_index_list), test_index_list, y_pred, pooled_document_count, prevalence)\n",
    "\n",
    "        # it means everything in the train list and we do not need to predict \n",
    "        # so we do not need any training of the model\n",
    "        if isPredictable.count(1) == 0:\n",
    "            break\n",
    "        # suppose original budget is 5\n",
    "        # then when train_index_list is 5, we cannot just turn off Active Learning\n",
    "        # we need to use that AL with train_index_list = 5 to train use that to predict the rest\n",
    "        # so we cannot exit at 5, we should exit at 5 + 1\n",
    "        # that is the reason we set per_topic_budget_from_trec_qrels[topicId] + 1 where 1 is the batch size\n",
    "        # it means everything of pooled_budget size is in the train_index_list so we need not tany training of the model\n",
    "        # so break here\n",
    "        if use_pooled_budget == 1 and per_topic_budget_from_trec_qrels[topicID] == len(train_index_list):\n",
    "            break\n",
    "\n",
    "        queueSize = isPredictable.count(1)\n",
    "        my_queue = queue.PriorityQueue(queueSize)\n",
    "\n",
    "        # these are used for SPL\n",
    "        randomArray = []\n",
    "\n",
    "        for counter in range(0, predicatbleSize):\n",
    "            if isPredictable[counter] == 1:\n",
    "                # model.predict returns a list of values in so we need index [0] as we\n",
    "                # have only one element in the list\n",
    "                y_prob = None\n",
    "                if feature_type == 'tfidf':\n",
    "                    y_prob = model.predict_proba(initial_X_test[counter])[0]\n",
    "                elif feature_type == 'bert' or feature_type == 'robert':\n",
    "                    y_prob = model.predict_proba(initial_X_test[counter])[0]\n",
    "\n",
    "                    val = 0\n",
    "                    if al_protocol == 'CAL':\n",
    "                        val = (-1)*y_prob[1] # -1 is needed priority do sorting increasing \n",
    "                        my_queue.put((val, counter))\n",
    "                    elif al_protocol == 'SAL':\n",
    "                        val = (-1)*calculate_entropy(y_prob[0], y_prob[1])\n",
    "                        my_queue.put((val, counter))\n",
    "                    elif al_protocol == 'SPL':\n",
    "                        randomArray.append(counter)\n",
    "        \n",
    "        if use_pooled_budget ==1:\n",
    "            size_limit = math.ceil(train_per_centage[loopCounter] * per_topic_budget_from_trec_qrels[topicID])\n",
    "        else:\n",
    "            size_limit = math.ceil(train_per_centage[loopCounter] * total_documents)\n",
    "        if al_protocol == 'SPL':\n",
    "            random.shuffle(randomArray)\n",
    "            batch_counter = 0\n",
    "            while True:\n",
    "                if train_size_controller == size_limit:\n",
    "                    break\n",
    "\n",
    "                itemIndex = randomArray[batch_counter]\n",
    "                isPredictable[itemIndex] = 0\n",
    "                train_index_list.append(test_index_dictionary[itemIndex])\n",
    "                train_size_controller += 1\n",
    "                batch_counter += 1\n",
    "        else:\n",
    "            while not my_queue.empty():\n",
    "                if train_size_controller == size_limit:\n",
    "                    break\n",
    "\n",
    "                item = my_queue.get()\n",
    "                # is a tuple where item[1] is the index, item[0] is the predict value\n",
    "                isPredictable[item[1]] = 0 # not predictable\n",
    "\n",
    "                train_index_list.append(test_index_dictionary[item[1]])\n",
    "                train_size_controller += 1\n",
    "\n",
    "        loopCounter += 1\n",
    "    return topic_all_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def active_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning(topic_list, df, al_protocol, al_classifier, document_collection, topic_seed_info, \n",
    "                    topic_complete_qrels_address, train_per_centage, data_path, file_name, use_pooled_budget, \n",
    "                    per_topic_budget_from_trec_qrels, feature_type):\n",
    "    num_workers = None\n",
    "    workers = ProcessPool(processes=1)\n",
    "    with tqdm(total=len(topic_list)) as pbar:\n",
    "        partial_active_learning_multi_processing = partial(active_learning_multi_processing, df=df, al_protocol=al_protocol, al_classifier = al_classifier,\n",
    "        document_collection=document_collection, topic_seed_info=topic_seed_info, topic_complete_qrels_address=topic_complete_qrels_address, \n",
    "        train_per_centage=train_per_centage, use_pooled_budget=use_pooled_budget, per_topic_budget_from_trec_qrels=per_topic_budget_from_trec_qrels, \n",
    "        feature_type=feature_type)\n",
    "        for topic_all_info in tqdm(workers.imap_unordered(partial_active_learning_multi_processing, topic_list)):\n",
    "            topicID = topic_all_info[0][0] # 0 is the loopCounter Index and 0 is the first tuple\n",
    "            file_complete_path = data_path + file_name + str(topicID) + '.pickle'\n",
    "            pickle.dump(topic_all_info, open(file_complete_path, \"wb\"))\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(s):\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    # Change 't to 'not'\n",
    "    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "    # Remove @name\n",
    "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
    "    # Isolate and remove punctuations except '?'\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Remove trailing whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = s.replace(\"https\", \"\")\n",
    "\n",
    "    s = re.compile('RT @').sub('@', s, count=1)\n",
    "    s = s.replace(\":\", \"\")\n",
    "    s = s.tp.clean(s)\n",
    "\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(ps.stem(item))\n",
    "    \n",
    "    return stems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
